{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_nb2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgvxXU_ExwH6",
        "colab_type": "code",
        "outputId": "de5dada7-5c98-42ed-9907-e72648ee8cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=231c4c3f7a3795b789f2baab0fe467203122d6f4056f88f1190dcf2949768945\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 11.7 GB  | Proc size: 2.8 GB\n",
            "GPU RAM Free: 10639MB | Used: 802MB | Util   7% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpvehfX9752z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C_oZjJxyCcr",
        "colab_type": "code",
        "outputId": "5f2d0388-b03b-4e1c-8f31-5953149e7dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1-71sf-yC_1",
        "colab_type": "code",
        "outputId": "4f008bd6-6ec5-4428-f435-bb55bf4a7aa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd drive/My Drive/drqa_doc_reader"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/drqa_doc_reader\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nHoqwmQDszp",
        "colab_type": "code",
        "outputId": "f6211dae-3e79-490a-a8db-8a2b8e00bf04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/drqa_doc_reader'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TUMojkcD0z5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install ujson"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5lFNcE_Dvfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!python setup.py --train_url=\"./data/train-v1.1.json\" --dev_url=\".data/dev-v1.1.json\" --glove_url=\".data/glove.840B.300d.zip\" --include_test_examples=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I56OUqUJH2WW",
        "colab_type": "code",
        "outputId": "1a8be0c6-5811-48ce-bf57-8bf72b6f849b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from model import StanfAR\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import time\n",
        "import spacy\n",
        "\n",
        "'''\n",
        "steps - \n",
        "1. load data\n",
        "2. preprocess\n",
        "3. train\n",
        "4. tensorboard / evaluation on dev\n",
        "5. saving/checkpointing/loading model\n",
        "6. predict function\n",
        "7. web app\n",
        "8. packaging, code quality testing, etc.\n",
        "'''\n",
        "#%%"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nsteps - \\n1. load data\\n2. preprocess\\n3. train\\n4. tensorboard / evaluation on dev\\n5. saving/checkpointing/loading model\\n6. predict function\\n7. web app\\n8. packaging, code quality testing, etc.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJBl-eRGIiA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_json_file(path):\n",
        "    with open(path) as file:\n",
        "        out = json.load(file)\n",
        "    return out\n",
        "\n",
        "\n",
        "def load_npz_file(path):\n",
        "    return np.load(path)\n",
        "\n",
        "\n",
        "def load_files(path):\n",
        "    word2idx = load_json_file(path + \"/word2idx.json\")\n",
        "    word_emb = load_json_file(path + \"/word_emb.json\")\n",
        "\n",
        "    train_data = load_npz_file(path + \"/train.npz\")\n",
        "    dev_data = load_npz_file(path + \"/dev.npz\")\n",
        "\n",
        "    idx2word = {i:j for j,i in word2idx.items()}\n",
        "\n",
        "    return word2idx, idx2word, word_emb, train_data, dev_data\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bOb5VjXIkux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% loading\n",
        "word2idx, idx2word, word_emb, train_data, dev_data = load_files(path='data')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AARxVbOJgP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOSffrhvJg1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnIsQZkwIro7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% preprocessing\n",
        "train_q = torch.LongTensor(train_data['ques_idxs']).to(device)\n",
        "train_c = torch.LongTensor(train_data['context_idxs']).to(device)\n",
        "\n",
        "labels1 = torch.as_tensor(train_data['y1s']).to(device)\n",
        "labels2 = torch.as_tensor(train_data['y2s']).to(device)\n",
        "\n",
        "word_emb = torch.as_tensor(word_emb).to(device)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a1DfmNLSb5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_q = torch.LongTensor(dev_data['ques_idxs']).to(device)\n",
        "dev_c = torch.LongTensor(dev_data['context_idxs']).to(device)\n",
        "\n",
        "labels1_dev = torch.as_tensor(dev_data['y1s']).to(device)\n",
        "labels2_dev = torch.as_tensor(dev_data['y2s']).to(device)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgJ6dpaSIo3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(data.Dataset):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.data = (train_q, train_c, labels1, labels2, dev_q, dev_c, labels1_dev, labels2_dev)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        query = self.data[0][idx]\n",
        "        ctx = self.data[1][idx]\n",
        "        y1 = self.data[2][idx]\n",
        "        y2 = self.data[3][idx]\n",
        "        \n",
        "        try:\n",
        "            dev_query = self.data[4][idx]\n",
        "            dev_ctx = self.data[5][idx]\n",
        "            dev_l1 = self.data[6][idx]\n",
        "            dev_l2 = self.data[7][idx]\n",
        "        except:\n",
        "            return query, ctx, y1, y2\n",
        "          \n",
        "        return query, ctx, y1, y2, dev_query, dev_ctx, dev_l1, dev_l2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA0QjDKFIs25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "df = torch.utils.data.DataLoader(Dataset(), batch_size=32)\n",
        "\n",
        "\n",
        "#%% training loop\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "network = StanfAR(word_emb, 32).to(device)\n",
        "\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
        "\n",
        "total_loss = 0\n",
        "total_correct = 0\n",
        "\n",
        "i = 0\n",
        "num_epochs = 500\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljV4icsm_na2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip install gputil\n",
        "# !pip install psutil\n",
        "# !pip install humanize\n",
        "# import psutil\n",
        "# import humanize\n",
        "# import os\n",
        "# import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C4ivyXItsYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17948da9-27b8-4024-b2d8-91133a75223f"
      },
      "source": [
        "i"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1865"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tviyp1S1IuLi",
        "colab_type": "code",
        "outputId": "0eb513a7-12eb-44aa-d53c-3ea5ab68a824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "max_acc = 0\n",
        "for j in range(num_epochs):\n",
        "    test_acc1 = []\n",
        "    test_acc2 = []\n",
        "    acc1 = []\n",
        "    acc2 = []\n",
        "    i = 0\n",
        "    tic_b = time.time()\n",
        "    test_done = False\n",
        "    for batch in df:  # Get Batch\n",
        "        i += 1\n",
        "        if j==0 and i<1865:\n",
        "            continue\n",
        "        try:\n",
        "            query, context, y1, y2, dev_q, dev_ctx, dev_y1, dev_y2 = batch\n",
        "        except:\n",
        "            query, context, y1, y2 = batch\n",
        "            test_done = True\n",
        "\n",
        "        if query.shape[0] != 32:\n",
        "            break\n",
        "\n",
        "        if i == 100:\n",
        "            toc_b = time.time()\n",
        "            print(f\"Time for 100 batches: {toc_b-tic_b}\")\n",
        "\n",
        "        preds = network(query, context)  # Pass Batch\n",
        "\n",
        "        loss = (F.cross_entropy(preds[0], y1))+(F.cross_entropy(preds[1], y2))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()  # Calculate Gradients\n",
        "        optimizer.step()  # Update Weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        acc1.append((preds[0].argmax(dim=1) == y1).sum().item())\n",
        "        acc2.append((preds[1].argmax(dim=1) == y1).sum().item())\n",
        "        \n",
        "        torch.save(network.state_dict(), \"doc_reader_state.pt\")\n",
        "\n",
        "        if not test_done:\n",
        "            with torch.no_grad():\n",
        "                test_preds1, test_preds2 = network(dev_q, dev_ctx)\n",
        "                accuracy1 = (test_preds1.argmax(dim=1)==dev_y1).sum().item()\n",
        "                accuracy2 = (test_preds2.argmax(dim=1)==dev_y2).sum().item()\n",
        "                test_acc1.append(accuracy1)\n",
        "                test_acc2.append(accuracy2)\n",
        "  \n",
        "    print(f\"Epoch: {j}\\ntrain_accuracy1: {np.mean(acc1[-100:])}\\ntrain_accuracy2: {np.mean(acc2[-100:])}\\ntest_accuracy1: {np.mean(test_acc1[-100:])}\\ntest_accuracy2: {np.mean(test_acc2[-100:])}\\n\")\n",
        "    \n",
        "    if np.mean(test_acc1[-100:]) + np.mean(test_acc2[-100:]) > max_acc:\n",
        "        max_acc = np.mean(test_acc1[-100:]) + np.mean(test_acc2[-100:])\n",
        "        torch.save(network.state_dict(), f\"doc_reader_state_{round(max_acc/2, 2)}.pth\")\n",
        "        print(\"model_saved\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "train_accuracy1: 20.2\n",
            "train_accuracy2: 7.68\n",
            "test_accuracy1: nan\n",
            "test_accuracy2: nan\n",
            "\n",
            "Time for 100 batches: 118.24767732620239\n",
            "Epoch: 1\n",
            "train_accuracy1: 20.34\n",
            "train_accuracy2: 7.91\n",
            "test_accuracy1: 15.29\n",
            "test_accuracy2: 17.12\n",
            "\n",
            "model_saved\n",
            "Time for 100 batches: 124.48526072502136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73sI5PeWuAJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = (np.mean(test_acc1[-100:]) + np.mean(test_acc2[-100:]))/2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XILFDpzktBBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(network.state_dict, f\"new_model{a}.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbJ6GVoXr7LU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c33d6966-0c54-4fcd-ce99-e349001b7ae8"
      },
      "source": [
        "i"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1865"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd6RUermr841",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuSOhBTwySCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to change model to allow for custom batch sizes and retrain. For now, can pass a dev batch and store it in dataframe to analyse model performance\n",
        "\n",
        "\n",
        "def batch2sent(batch, colname):\n",
        "    return pd.DataFrame(map(lambda x: ' '.join([idx2word[i] for i in x]), batch.tolist()), columns=[colname])\n",
        "  \n",
        "def ans2sent(y, colname):\n",
        "    return pd.DataFrame(map(lambda x: idx2word[x], y.tolist()), columns = [colname])\n",
        "\n",
        "def concat_cols(df1, df2):\n",
        "    return pd.concat([df1, df2], axis=1)\n",
        "  \n",
        "def concat_rows(df1, df2):\n",
        "    return pd.concat([df1, df2], axis=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMQj2yCESBF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "i=0\n",
        "output_df = pd.DataFrame()\n",
        "for batch in df:\n",
        "    if i==10:\n",
        "        break\n",
        "    else:\n",
        "        query, context, y1, y2, dev_q, dev_ctx, dev_y1, dev_y2 = batch\n",
        "        with torch.no_grad():\n",
        "            test_preds1, test_preds2 = network(dev_q, dev_ctx)\n",
        "        \n",
        "        query_df = batch2sent(dev_q, \"query\")\n",
        "        ctx_df = batch2sent(dev_ctx, \"context\")\n",
        "        actual_start = ans2sent(dev_y1, dev_ctx \"actual_start\")\n",
        "        actual_end = ans2sent(dev_y2, \"actual_end\")\n",
        "        pred_start = ans2sent(test_preds1.argmax(dim=1), \"pred_start\")\n",
        "        pred_end = ans2sent(test_preds2.argmax(dim=1), \"pred_end\")\n",
        "        start_same = pd.Series(test_preds1.argmax(dim=1).tolist())==pd.Series(dev_y1.tolist())\n",
        "        end_same = pd.Series(test_preds2.argmax(dim=1).tolist())==pd.Series(dev_y2.tolist())\n",
        "        \n",
        "        out = pd.concat([query_df, ctx_df, actual_start, actual_end, pred_start, pred_end, start_same, end_same], axis=1)\n",
        "        \n",
        "        output_df = pd.concat([output_df, out], axis=0)\n",
        "         \n",
        "        \n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbJnIbusUrsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_df.to_excel(\"predictions2.xlsx\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PwAMU9viy-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}